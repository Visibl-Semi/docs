---
title: "Visibl LLM Service"
description: "Enterprise-grade local AI infrastructure for silicon engineering teams"
icon: "server"
---


The Visibl LLM Service provides secure, high-performance AI infrastructure specifically designed for silicon engineering workloads. It runs entirely on your hardware, ensuring complete data isolation while delivering GPU-optimized performance for RTL design, verification, and EDA tool integration.

<Info>
**Complete Security**: All AI processing happens locally on your infrastructure. No code, designs, or data ever leave your network, making it perfect for sensitive silicon engineering projects.
</Info>

## Why local AI infrastructure?

**Data Security**: Complete data isolation with zero external API calls. Perfect for classified and proprietary silicon designs.

**High Performance**: GPU-optimized models with tensor parallelism deliver low-latency responses for complex engineering queries.

**Full Control**: Deploy on your infrastructure with complete control over models, data, and performance characteristics.

## Model options

Choose the optimal model size for your silicon engineering workload:

### Visibl Fast (7B)

**Single GPU Deployment**

- **Hardware**: 24GB+ VRAM (RTX 4090, A6000, A100)
- **Latency**: `<2s` response time for most queries  
- **Use Cases**: Code completion, syntax help, quick debugging
- **Throughput**: High concurrent request handling

**Ideal for**: Daily development tasks, interactive coding assistance

### Visibl Heavy (70B)

**Multi-GPU Deployment**

- **Hardware**: 2x A100-40GB or 4x RTX 4090 (recommended)
- **Latency**: 5-15s for complex multi-step reasoning
- **Use Cases**: Architecture decisions, complex debugging, formal verification
- **Throughput**: Lower concurrency, higher reasoning quality

**Ideal for**: Complex design problems, architectural planning

<Tip>
**Model Selection**: Start with the Fast model for most workflows. The Heavy model excels at complex multi-step reasoning tasks like architectural planning and formal verification setup.
</Tip>

## System requirements

### Minimum requirements

For development and testing environments:

| Component | Specification |
|-----------|--------------|
| **OS** | RHEL 8.x, Ubuntu 20.04+, or compatible Linux |
| **Docker** | Version 20.10+ with GPU support |
| **GPU** | NVIDIA A100, H100, or RTX 4090 with 24GB+ VRAM |
| **RAM** | 64GB+ system memory |
| **Storage** | 100GB+ available space (SSD recommended) |
| **Network** | Outbound HTTPS for initial model download |

<Info>
**GPU Compatibility**: Requires NVIDIA GPUs with compute capability 7.0+ and CUDA 11.8+ support
</Info>

### Production recommendations

For enterprise production deployments:

| Component | Specification |
|-----------|--------------|
| **OS** | RHEL 8.x (certified and supported) |
| **Docker** | Docker CE/EE 24.x with NVIDIA Container Runtime |
| **GPU** | 2x NVIDIA A100-40GB or 4x RTX 4090 |
| **RAM** | 128GB+ DDR4/DDR5 system memory |
| **Storage** | 500GB+ NVMe SSD for models and cache |

## Enterprise features

### Security & compliance

- **Air-gapped deployment** - no internet required after setup
- **Zero telemetry** - no usage statistics sent externally  
- **Complete data isolation** - all processing on your hardware
- **Network isolation** ready for classified environments
- **Audit logging** for compliance requirements
- **Docker container isolation** for process security

```bash Security verification
# Verify no external network access
sudo netstat -tuln | grep :58180
# Only local bindings should be shown

# Check container isolation
docker inspect visibl-llm | grep NetworkMode
```

### Production deployment

- **Systemd integration** for reliable service management
- **Health monitoring** with automatic recovery mechanisms
- **Resource management** with intelligent memory allocation
- **Load balancing** support for high availability setups
- **Backup and recovery** procedures for model and config data
- **Rolling updates** with zero downtime deployment

```bash Service management
# Production service control
sudo systemctl enable visibl-llm
sudo systemctl start visibl-llm
sudo systemctl status visibl-llm

# Monitor service health
visibl-llm status
visibl-llm logs -f
```

### Performance optimization

- **Tensor parallelism** for large model distribution
- **Automatic scaling** based on concurrent request load
- **Memory optimization** for efficient GPU utilization
- **Request batching** for improved throughput
- **Model caching** for frequently accessed weights
- **Custom quantization** support (AWQ, GPTQ)

```bash Performance tuning
# Optimize GPU memory utilization
visibl-llm util 0.90

# Enable custom all-reduce for multi-GPU
visibl-llm custom-ar on

# Adjust context length for workload
visibl-llm len 32000
```

## Monitoring & observability

The service includes comprehensive monitoring capabilities:

### Health endpoints

- `/health` - Service status and model info
- `/metrics` - Performance and usage metrics  
- `/v1/models` - Available model information
- Real-time status monitoring

### Logging & diagnostics

- Request/response logging
- Performance metrics tracking
- Error diagnostics and stack traces
- Integration with systemd journal

## Ready to deploy?

<CardGroup cols={2}>
  <Card title="Installation Guide" icon="download" href="/llm-service/installation">
    Step-by-step deployment instructions with automated installer
  </Card>
  
  <Card title="Usage Guide" icon="book-open" href="/llm-service/usage">
    Service management, monitoring, and optimization best practices
  </Card>
  
  <Card title="CLI Integration" icon="terminal" href="/cli/overview">
    Learn how to connect the CLI tool to your LLM service
  </Card>
  
  <Card title="Get Support" icon="messages-square" href="mailto:support@visiblsemi.com">
    Contact our team for technical support and custom deployment assistance
  </Card>
</CardGroup>

## Support

Need custom deployment assistance? Our team provides enterprise deployment support, including air-gapped installations, custom hardware configurations, and integration with existing infrastructure.

Contact us at [support@visiblsemi.com](mailto:support@visiblsemi.com) for enterprise support.