---
title: "LLM Service"
description: "Local AI infrastructure for silicon engineering"
---

The Visibl LLM Service is the AI infrastructure that powers the Visibl CLI. Install once on your server, then interact through the CLI from your workstations.

<Info>
**Set it and forget it**: The LLM service runs on your infrastructure as a background service. Once installed, users interact through the Visibl CLIâ€”not directly with the LLM service.
</Info>

## Why Local AI?

**Security**: All processing happens locally. No code or designs leave your network.

**Performance**: GPU-optimized for low-latency responses.

**Control**: Full control over deployment and data.

## Model Options

**Visibl Max-2 (Default)**
- GPU: 2x A100-40GB
- Context: 128K tokens
- Use case: Agentic workflows, large codebases

**Visibl Fast**
- GPU: 24GB+ VRAM
- Context: 64K tokens
- Use case: Interactive development, low latency

**Visibl Expert**
- GPU: 2x A100-40GB
- Context: 128K tokens
- Use case: Complex reasoning, extended context

**Visibl Heavy**
- GPU: 2x A100-40GB or 4x RTX 4090
- Context: 128K tokens
- Use case: Maximum reasoning capability

## System Requirements

### Minimum (Fast model)

| Component | Specification |
|-----------|--------------|
| **OS** | RHEL 8.x, Ubuntu 20.04+ |
| **Docker** | Version 26.x |
| **GPU** | NVIDIA 24GB+ VRAM |
| **RAM** | 64GB+ |
| **Storage** | 100GB+ SSD |

### Production (Max-2/Expert/Heavy)

| Component | Specification |
|-----------|--------------|
| **OS** | RHEL 8.x |
| **Docker** | Version 26.x |
| **GPU** | 2x NVIDIA A100-40GB |
| **RAM** | 128GB+ |
| **Storage** | 500GB+ NVMe SSD |

## Next Steps

<CardGroup cols={2}>
  <Card title="Installation" icon="download" href="/llm-service/installation">
    Install the LLM service on your server
  </Card>
  
  <Card title="Usage" icon="book-open" href="/llm-service/usage">
    Manage and switch models
  </Card>
  
  <Card title="CLI" icon="terminal" href="/cli/overview">
    Install the CLI to start working
  </Card>
</CardGroup>
